---
layout: post
title: "ONNX-opt day0"
date:   2025-9-11
tags: [dev, nn, onnx, ai-compiler]
comments: true
author: 久菜合子
---

今天是人类目前最大的湿件制导导弹命中目标的24(man!)周年

##### ONNX 简介
ONNX 是一个开放的深度学习框架中立的表示格式，旨在促进不同深度学习工具之间的互操作性。通过 ONNX，开发者可以在不同的深度学习框架之间轻松地转换模型，从而实现更高效的模型部署和推理。<br>
ONNX 和 ONNX-runtime 是两个重要的组成部分，前者是模型计算图和权重的中间表示，后者是一个推理引擎，如下图。<br>
![ONNX 和 ONNX-runtime 在整个AI生态中的位置](../_post_imgs/ONNX_opt-0.png)
所以 ONNX 主要用于 AI 模型的交换和部署：
- 交换：对不同训练框架导出模型文件和权重文件进行转换
- 部署：将模型文件和权重文件交给推理引擎进行运算

上图中的三段式和传统编译器的结构不谋而和，ONNX 虽然并不存在计算图或者算子上的简化，但是其中的一些结构对于模型的抽象使得优化其实不是完全不可能（进入编译器舒适区间了）<br>
比如，现在已有的 <a href="https://github.com/daquexian/onnx-simplifier?tab=readme-ov-file">ONNX-simplifier</a>（虽然不是很流行），根据其描述是进行计算图上的常量折叠<br>，以及他的third-part依赖<a href="https://github.com/onnx/optimizer">ONNX-optimizer</a>
相较于如何优化这方面，也很重要的方面是如何对 Pass 后的结果进行验证，下面是一些主流的 AI 编译器的测试方法（gen by Gemini）

| 框架/标准 | 主要验证策略                                   | 核心工具/方法                                                       |
| --------- | ---------------------------------------------- | ------------------------------------------------------------------- |
| ONNX      | 结构合法性验证、数值一致性对比                 | onnx.checker、ONNX Runtime（对比不同优化级别/后端）、模型库测试     |
| MLIR      | 逐层方言规约验证、Pass正确性断言               | Dialect Verifier、mlir-opt + FileCheck、端到端数值对比              |
| TVM       | 端到端数值对比、中间层IR对比、底层代码单元测试 | Relay解释器（对比优化前后）、与NumPy对比、端到端与原始框架对比      |
| Triton    | 与参考实现的数值一致性对比、梯度检查           | 与PyTorch Eager实现的输出对比、torch.autograd.gradcheck进行梯度验证 |

相比之下，ONNX 有一些优势，提供了足够的`语法与结构规约验证(Syntactic and Structural Verification)`的 Python API，不必一定要进行`数值一致性对比 (Numerical Consistency Check)`, 虽然后者可以通过 ONNX Runtime 来实现<br>
不过`数值一致性对比`还是很有必要的，但是对规模比较大的测例而言，算力也是一个问题<br>
ONNX 的另一个优点在于，它不像是 TVM 进行端到端的训练和部署，所以研究性的工作可能会比轻松<br>

##### ONNX 环境配置
由于是进行研究，所以需要一套源代码 <a href="https://github.com/onnx/onnx/releases/tag/v1.19.0">ONNX 1.19.0 release</a> <br>
再次之前，墙裂建议使用`conda`等工具进行环境隔离<br>
在编译之前，需要安装`protobuf`，以下是源代码编译安装
```bash
  git clone https://github.com/protocolbuffers/protobuf.git
  cd protobuf
  git checkout v5.29.2
  git submodule update --init --recursive
  mkdir build_source && cd build_source
  cmake -Dprotobuf_BUILD_SHARED_LIBS=OFF -DCMAKE_INSTALL_PREFIX=/usr -Dprotobuf_BUILD_TESTS=OFF -DCMAKE_BUILD_TYPE=Release -DCMAKE_POSITION_INDEPENDENT_CODE=ON ..
  cmake --build . --target install
```
`protobuf`是一个关键的序列化工具，之后的源代码中的部分`.cc`和`.h`文件是由其生成的
然后在主文件夹下使用`pip install -e . -v`就能编译安装，python 包管理器会将 onnx 关联到当前路径<br>
不过，毕竟需要看代码，所以`cmake`相关的需要进行一点变动：
- 在`CMakeLists.txt`中，加入一行`set(CMAKE_EXPORT_COMPILE_COMMANDS TRUE)
`, 以便生成`compile_commands.json`, 让clangd作为看c++时的lsp
- 使用 `cmake -DENABLE_FASTER_BUILD=OFF .`, 如果开启快速编译，某些由`protobuf`生成的文件可能不会保存
  
在生成的`onnx-ml.pb.h`中，lsp可能无法解析符号`PROTOBUF_NODISCARD`, <a href="https://github.com/protocolbuffers/protobuf/commit/1ceedf88ca4c6f151f08a10244005cee6c814f40">根据这个commit</a>，应该可以将其替换为编译器自带的`[[nodiscard]]`<br>
`onnx-ml.pb.h`会校验c++版本，由于只是需要其提供定义和符号，所以也将其注释<br>
```c++
#if PROTOBUF_VERSION != 5029002
#error "Protobuf C++ gencode is built with an incompatible version of"
#error "Protobuf C++ headers/runtime. See"
#error "https://protobuf.dev/support/cross-version-runtime-guarantee/#cpp"
...
...
#endif // ps: 这个endif在文件末尾
```

最后, 为了进行数据一致性检验，可能需要下一个onnx runtime<br>

##### ONNX项目组成结构
由于国内（其实也包括国外），ONNX 的资料不算很多，近年来有些式微，很多训练框架并不愿意全盘兼容ONNX，导致ONNX主打的框架迁移用处不大，然后 ONNX-runtime也不算特别出色。<br>
项目树（仅文件夹）:
```bash
$ tree onnx -d
├── bin
├── common
├── defs
│   ├── controlflow
│   ├── generator
│   ├── image
│   ├── logical
│   ├── math
│   ├── nn
│   ├── object_detection
│   ├── optional
│   ├── __pycache__
│   ├── quantization
│   ├── reduction
│   ├── rnn
│   ├── sequence
│   ├── tensor
│   ├── text
│   ├── traditionalml
│   └── training
├── frontend
├── inliner
├── onnx_cpp2py_export
├── reference
```
带`.pb`的是由`protobuf`导出的c++文件，带`_pb`的是导出的python文件
- `bin`: 存放了一个`checker.py`, 方便导出
- `common`: c++ 编写的实用程序，如 assert，file，path，platform等, 其中有不少的是`Highly Experimental`的
- `defs`: 存放 ONNX 对于模型各个部件的定义，说实话从技术的角度上没什么好看的
- `frontend`: 似乎是空的
- `onnx_cpp2py_export` 以及 `onnx_cpp2py_export...so`: cpp 和 py 之间进行接口交换
- `reference`: 定义了大量的算子，绝大多数是用`numpy`实现的

##### ONNX-optimizer
ONNX-optimizer 以 ONNX 作为依赖，配置基本如上所示<br>
ONNX-optimizer 显然更有研究的价值，虽然它的star量大概是其他 AI 编译器的百分之一，贡献者也只有几十人<br>
ONNX-optimizer 使用 ONNX 作为 third-part, 也就是应该首先按照上述方法构建ONNX的环境，否则lsp应该是没法使用的.<br>
首先是文件树：
```bash
onnxoptimizer
├── c_api
│   ├── onnxoptimizer_c_api.cc
│   └── onnxoptimizer_c_api.h
├── cpp2py_export.cc
├── __init__.py
├── __main__.py
├── model_util.cc
├── model_util.h
├── onnxoptimizer_main.py
├── optimize.cc
├── optimize.h
├── pass.cc
├── passes
│   ├── adjust_add.h
│   ├── adjust_slice_and_matmul.h
│   ├── bitscast.h
│   ├── cse_util.h
│   ├── data_type.h
│   ├── eliminate_common_subexpression.h
│   ├── eliminate_consecutive_idempotent_ops.h
│   ├── eliminate_deadend.h
│   ├── eliminate_duplicate_initializer.h
│   ├── eliminate_identity.h
│   ├── eliminate_if_with_const_cond.h
│   ├── eliminate_nop_cast.h
│   ├── eliminate_nop_concat.h
│   ├── eliminate_nop_dropout.h
│   ├── eliminate_nop_expand.h
│   ├── eliminate_nop_flatten.h
│   ├── eliminate_nop_monotone_argmax.h
│   ├── eliminate_nop_pad.h
│   ├── eliminate_nop_reshape.h
│   ├── eliminate_nop_split.h
│   ├── eliminate_nop_transpose.h
│   ├── eliminate_nop_with_unit.h
│   ├── eliminate_shape_gather.h
│   ├── eliminate_shape_op.h
│   ├── eliminate_slice_after_shape.h
│   ├── eliminate_unused_initializer.h
│   ├── extract_constant_to_initializer.h
│   ├── fuse_add_bias_into_conv.h
│   ├── fuse_bn_into_conv.h
│   ├── fuse_concat_into_reshape.h
│   ├── fuse_consecutive_concats.h
│   ├── fuse_consecutive_log_softmax.h
│   ├── fuse_consecutive_reduce_unsqueeze.h
│   ├── fuse_consecutive_slices.h
│   ├── fuse_consecutive_squeezes.h
│   ├── fuse_consecutive_transposes.h
│   ├── fuse_consecutive_unsqueezes.h
│   ├── fuse_matmul_add_bias_into_gemm.h
│   ├── fuse_pad_into_conv.h
│   ├── fuse_pad_into_pool.h
│   ├── fuse_qkv.h
│   ├── fuse_transpose_into_gemm.h
│   ├── lift_lexical_references.h
│   ├── logging.h
│   ├── nop.h
│   ├── pass_util.cc
│   ├── pass_util.h
│   ├── rename_input_output.h
│   ├── replace_einsum_with_matmul.h
│   ├── rewrite_input_dtype.h
│   ├── set_unique_name_for_nodes.h
│   ├── split.h
│   ├── string_utils.h
│   ├── tensor_util.cc
│   └── tensor_util.h
├── pass.h
├── pass_manager.cc
├── pass_manager.h
├── pass_registry.cc
├── pass_registry.h
└── test
    └── optimizer_test.py
```
可以看出项目比较紧凑（小）
了解构建模式之前，可以先看看给出的使用示例
```c++
// onnx_optimizer_exec.cpp

/*
 * SPDX-License-Identifier: Apache-2.0
 */
...

  try {
    ONNX_NAMESPACE::ModelProto model;
    onnx::optimization::loadModel(&model, model_in_path, true);
    onnx::checker::check_model(model);
    auto new_model = onnx::optimization::Optimize(
        model, onnx::optimization::GetFuseAndEliminationPass());
    onnx::checker::check_model(new_model);
    bool save_external_data = !model_data_path.empty();
    onnx::optimization::saveModel(&new_model, model_out_path,
                                  save_external_data, model_data_path);

  } catch (std::exception& e) {
    std::cout << e.what() << std::endl;
    return -1;
  }
  return 0;

...

```
- `ModelProto`: 继承于`::google::protobuf::Message`, 用于模型的存储，修改以及基于`protobuf`进行序列化, 由ONNX提供
- `loadModel`: ONNX-opt提供，当true时，在模型文件同文件夹下寻找并加载额外文件（权重），并调用`loadExternalDataForTensor`，转化为tensor形式
- `check_model`：按照ONNX标准进行检查
- `saveModel`: 保存模型和权重
- `onnx::optimization::Optimize/OptimizeFixed`: 优化过程分为常规优化和不动点优化，这两个接口将会被导出到python（需安装pybind11）


